{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802efd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Union\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e4b4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WindowDataset(Dataset):\n",
    "    \"\"\"Slice a feature parquet/CSV table into sliding windows for multi-task crypto prediction.\n",
    "\n",
    "    Each sample i consists of:\n",
    "        • X  :  history_steps × n_features (float32)\n",
    "        • y  : dict with keys 'cls', 'p90', 'p10', 'sigma' (tensor)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path              : Path to parquet or csv produced by make_targets.py (must contain targets).\n",
    "    history_steps     : Number of rows in the historical window (e.g. 90 d @ 15-min = 8640).\n",
    "    stride            : Step size (rows) to slide the window. 96≈1 day; 1 for full overlap.\n",
    "    split             : One of 'train', 'val', 'test' — dataset will filter according to `split_ranges`.\n",
    "    split_ranges      : Dict {\"train\": (start_dt, end_dt), ...}. Dates in pandas-parseable string or pd.Timestamp.\n",
    "    feature_cols      : Optional[List[str]] — if None, use all columns minus targets.\n",
    "    cache_in_memory   : Whether to keep numpy arrays in memory (speed vs RAM).\n",
    "    \"\"\"\n",
    "\n",
    "    TARGETS = {\n",
    "        \"cls\": np.int64,\n",
    "        \"p90\": np.float32,\n",
    "        \"p10\": np.float32,\n",
    "        \"sigma\": np.float32,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: Union[str, Path],\n",
    "        history_steps: int = 8640,\n",
    "        stride: int = 96,\n",
    "        split: str = \"train\",\n",
    "        split_ranges: Optional[Dict[str, tuple]] = None,\n",
    "        feature_cols: Optional[List[str]] = None,\n",
    "        cache_in_memory: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.history = history_steps\n",
    "        self.stride = stride\n",
    "\n",
    "        # 1. load\n",
    "        path = Path(path)\n",
    "        if path.suffix == \".parquet\":\n",
    "            df = pd.read_parquet(path)\n",
    "        elif path.suffix == \".csv\":\n",
    "            df = pd.read_csv(path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file type: %s\" % path.suffix)\n",
    "\n",
    "        # 2. ensure datetime index exists\n",
    "        if \"datetime\" in df.columns:\n",
    "            df[\"datetime\"] = pd.to_datetime(df[\"datetime\"], utc=True)\n",
    "            df.set_index(\"datetime\", inplace=True)\n",
    "        elif \"time\" in df.columns:  # seconds\n",
    "            df.index = pd.to_datetime(df[\"time\"], unit=\"s\", utc=True)\n",
    "        else:\n",
    "            raise KeyError(\"Input table must contain 'datetime' or 'time' column.\")\n",
    "\n",
    "        # 3. select split by date range\n",
    "        if split_ranges is not None:\n",
    "            start, end = split_ranges[split]\n",
    "            df = df.loc[start:end]\n",
    "\n",
    "        df.sort_index(inplace=True)\n",
    "\n",
    "        # 4. drop rows with any NaN in targets or features\n",
    "        self.df = df.dropna(how=\"any\").reset_index(drop=True)\n",
    "\n",
    "        # 5. choose feature columns\n",
    "        all_target_cols = list(self.TARGETS.keys()) + [\"r90\", \"sigma30_real\"]  # r90 may be needed in loss\n",
    "        if feature_cols is None:\n",
    "            feature_cols = [c for c in self.df.columns if c not in all_target_cols]\n",
    "        self.feat_cols = feature_cols\n",
    "\n",
    "        # 6. numpy cache\n",
    "        X = self.df[self.feat_cols].to_numpy(dtype=np.float32)\n",
    "        y = {t: self.df[t].to_numpy(dtype=dtype) for t, dtype in self.TARGETS.items()}\n",
    "\n",
    "        if cache_in_memory:\n",
    "            self.X_mem = X\n",
    "            self.y_mem = y\n",
    "        else:\n",
    "            self.X_mem = None\n",
    "            self.y_mem = None\n",
    "\n",
    "        # 7. precompute window end indices\n",
    "        first_idx = self.history - 1\n",
    "        last_idx = len(self.df) - 1\n",
    "        self.ends = np.arange(first_idx, last_idx + 1, self.stride, dtype=np.int64)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.ends)\n",
    "\n",
    "    def _get_slice(self, end_idx: int):\n",
    "        start = end_idx - self.history + 1\n",
    "        if self.X_mem is not None:\n",
    "            x = self.X_mem[start : end_idx + 1]\n",
    "        else:\n",
    "            x = self.df.iloc[start : end_idx + 1][self.feat_cols].to_numpy(np.float32)\n",
    "        return x\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        end = self.ends[idx]\n",
    "        x = self._get_slice(end)\n",
    "        target = {\n",
    "            \"cls\": torch.tensor(self.df.at[end, \"cls\"], dtype=torch.long),\n",
    "            \"p90\": torch.tensor(self.df.at[end, \"p90\"], dtype=torch.float32),\n",
    "            \"p10\": torch.tensor(self.df.at[end, \"p10\"], dtype=torch.float32),\n",
    "            \"sigma\": torch.tensor(self.df.at[end, \"sigma30_real\"], dtype=torch.float32),\n",
    "        }\n",
    "        return {\n",
    "            \"inputs\": torch.from_numpy(x),\n",
    "            **target,\n",
    "        }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
